
6/3/22

After creating a network that achieves 90% win rate on a 6x6 board, I decided to try various changes to improve the win rate. First, I tried training a policy network, but this was more difficult than imagined, and since the action space in Snake is so small, the policy network will not be much help in training the network.

I tried to standardize the learning routine by decreasing the learning rate in inverse proportion to the current maximum score. This proved an effective way to approach altering the learning rate. Through experimentation, and perhaps contrary to intuition, I found that the learning rate should decrease faster than in inverse proportion to the current maximum score.

Now, I'm trying an unfolding technique to both speed up the training process and provide a more effective tree search. This is perhaps the most promising technique so far.


6/4/22

After a perfect run on my 6x6 snake, I decided to expand to 10x10 snake. This posed many challenges. My first attempt network was to 6x10x10 (5x5) 9x10x10 (5x5) 9x5x5 -> 150 -> 1. This network does not work very effectively. Perhaps it is my function for sampling actions that is off. Or perhaps it is that my network is not large enough. I'll try to diagnose the problem first and then go from there.

After printing a game with action probabilities, it seems that the action sampling is not the biggest concern. The network design may be at the most fault here.


6/6/22

Notable game on 10x10 board: fastest way to lose

81,3,0,0,1,2,1,2,2,1,2,1,0,60,1,2,3,0,3,2,81,0,3,2,3,3,0,3,2


6/10/22

TURN OFF SANITIZER. It practically doubles run time.


6/11/22

Network I decided:
(5x5) 6x10x10 (5x5) 9x10x10 (pool) 9x5x5 (5x5) 9x5x5 -> 150 -> 1
Possible candidate:
(7x7) 10x8x8 (5x5) 10x8x8 (5x5) 15x8x8 (pool) 15x4x4 -> 200 -> 1

Note: Learning rate is extremely important, and it is best to tune this value for different networks. For the network I decided on going on, 0.002 is too large. I trained the program overnight with learning rate 0.0007 and it worked fairly well, giving an average score of 41. Now I'm trying an even smaller rate of 0.0003.

6/12/22
ALWAYS REMEMBER TO CAFFEINATE.


6/13/22

I tried training the candidate network from scratch, but it didn't work. I guess it is just hard to train networks from scratch.

Training it using 100 games played by the deterministic algorithm proved more effective.

Compiling command:
g++ main.cpp data.cpp convNet.cpp environment.cpp InputLayerCode.cpp trainer.cpp

Then:
./a.out

6/16/22
SNAIL: System for Neural Autonomous Iterative Learning


6/19/22
Game 163: first 10x10 snake solve.


6/20/22

A slight problem with the reward-based learning framework: when I remove single-action states, the states skipped between the states should be counted as extra time steps in the value formula. Should raise discountFactor to the power of the time difference between states.

This bug actually leads to an interesting strategy by the agent. For example, Game 921 Timer 199. Instead of going directly for the apple, the agent opts for moving away and letting the forced moves carry it closer to the apple.


6/21/22

Analogies: AI declaring war on AI. Humans get in the way of the antimatter bombs. Short-term goals are easier than long-term goals.


6/22/22

Large batch size may lead to overfitting. Perhaps it is necessary to decrease numBatches.


6/27/22

Fixed bug where final layer applies ReLu.


6/30/22

Wondering how explorationConstant affects training. Before, I set explorationConstant=1, but I'll try setting it to 0.5 to see how things change.

SEEMS LIKE IT WORKS. Pay close attention to explorationConstant.


7/25/22

Resolving issues with Git LFS:
https://github.blog/2017-06-27-git-lfs-2-2-0-released/


7/27/22

It seems that increasing batchSize to 1000 doesn't affect the effectiveness of the training. batchSize=100 seems to be the best way to go for discountFactor=0.98.

9/3//22

With Threads:

g++ -std=c++11 -pthread main.cpp data.cpp convNet.cpp environment.cpp InputLayerCode.cpp trainer.cpp

9/28/22

To remove a directory, use

rm -r -f some_folder

To push a folder, use

rsync -r some_folder username@host:./folder

To request a cluster node, use 

srun --pty -p test --mem 100 -t 0-01:00 /bin/bash

10/12/22

Compiling with optimizations

g++ -O2 -std=c++11 -pthread main.cpp data.cpp convNet.cpp environment.cpp InputLayerCode.cpp trainer.cpp common.cpp nash.cpp MCTS.cpp

10/23/22

Testing initial multiagent setting

g++ -O2 -std=c++11 -pthread main.cpp environment.cpp convNet.cpp InputLayerCode.cpp

rsync -r multiagent_snake kevindu@login.rc.fas.harvard.edu:./MultiagentSnake
rsync -r kevindu@login.rc.fas.harvard.edu:./MultiagentSnake/multiagent_snake .

2/5/23

srun --pty -p bigmem --mem 200000 -t 2-00:00 /bin/bash

Adversarial without storing policy: 7.95 per game out of 800
Adversarial with storing policy: 

For switching to FASRC system:

change NUM_THREADS
uncomment abs function
change evalPeriod

2/23/23

Changed explorationConstant to cUCB

2/28/23

3/7/23 Submitted job 44890129
3/8/23 Submitted job 44932277

8/2/23

g++ -O2 -std=c++11 -pthread main.cpp data.cpp convNet.cpp environment.cpp InputLayerCode.cpp trainer.cpp common.cpp nash.cpp MCTS.cpp kmeans.cpp

10/23/23
g++ -O2 -std=c++11 -pthread main.cpp data.cpp environment.cpp trainer.cpp common.cpp nash.cpp MCTS.cpp kmeans.cpp

11/13/23
g++ -O2 -std=c++11 -pthread main.cpp data.cpp environment.cpp trainer.cpp common.cpp nash.cpp MCTS.cpp kmeans.cpp convNet.cpp InputLayerCode.cpp -I "/Users/kevindu/Desktop/Employment/Multiagent Snake Research/multiagent_snake/LSTM" LSTM/model.cpp LSTM/PVUnit.cpp LSTM/layer.cpp LSTM/layers/lstmlayer.cpp LSTM/layers/policy.cpp LSTM/layers/conv.cpp LSTM/layers/pool.cpp LSTM/params.cpp LSTM/node.cpp

g++ -O2 -std=c++11 -pthread main.cpp environment.cpp MCTS.cpp trainer.cpp common.cpp nash.cpp kmeans.cpp convNet.cpp InputLayerCode.cpp -I "/Users/kevindu/Desktop/Employment/Multiagent Snake Research/multiagent_snake/LSTM" LSTM/model.cpp LSTM/PVUnit.cpp LSTM/layer.cpp LSTM/layers/lstmlayer.cpp LSTM/layers/policy.cpp LSTM/layers/conv.cpp LSTM/layers/pool.cpp LSTM/params.cpp LSTM/node.cpp

-fsanitize=address -fsanitize=undefined -fno-sanitize-recover=all -fsanitize=float-divide-by-zero -fsanitize=float-cast-overflow -fno-sanitize=null -fno-sanitize=alignment

12/17/23

BUG: when running locally, make sure maxStates is small. Otherwise you get an error:
AddressSanitizer:DEADLYSIGNAL
=================================================================
==49781==ERROR: AddressSanitizer: stack-overflow on address 0x00016b5a8fc0 (pc 0x0001026c8a54 bp 0x00018b4c1430 sp 0x00016b5a8fa0 T0)
    #0 0x1026c8a54 in main+0x7c (a.out:arm64+0x100004a54)

SUMMARY: AddressSanitizer: stack-overflow (a.out:arm64+0x100004a54) in main+0x7c
==49781==ABORTING
zsh: abort      ./a.out

